-- Databricks notebook source
-- MAGIC %md
-- MAGIC
-- MAGIC <div style="text-align: center; line-height: 0; padding-top: 9px;">
-- MAGIC   <img
-- MAGIC     src="https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png"
-- MAGIC     alt="Databricks Learning"
-- MAGIC   >
-- MAGIC </div>
-- MAGIC

-- COMMAND ----------

-- MAGIC %md
-- MAGIC # 3 - Adding Data Quality Expectations
-- MAGIC
-- MAGIC In this demonstration we will add data quality expectations to apply quality constraints that validates data as it flows through Lakeflow Spark Declarative Pipelines. Expectations provide greater insight into data quality metrics and allow you to fail updates or drop records when detecting invalid records.
-- MAGIC
-- MAGIC
-- MAGIC ### Learning Objectives
-- MAGIC
-- MAGIC By the end of this lesson, you will be able to:
-- MAGIC - Add quality constraints within a Lakeflow Spark Declarative Pipeline to trigger appropriate actions (warn, drop, or fail) based on data expectations.
-- MAGIC - Analyze pipeline metrics to identify and interpret data quality issues across different data flows.

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Setup
-- MAGIC

-- COMMAND ----------

-- DBTITLE 1,Create_declarative_function
-- MAGIC %python
-- MAGIC import json
-- MAGIC import os
-- MAGIC import json
-- MAGIC from databricks.sdk import WorkspaceClient
-- MAGIC
-- MAGIC
-- MAGIC def create_declarative_pipeline(pipeline_name: str, 
-- MAGIC                         root_path_folder_name: str,
-- MAGIC                         source_folder_names: list = [],
-- MAGIC                         catalog_name: str = 'dbacademy',
-- MAGIC                         schema_name: str = 'default',
-- MAGIC                         serverless: bool = True,
-- MAGIC                         configuration: dict = {},
-- MAGIC                         continuous: bool = False,
-- MAGIC                         photon: bool = True,
-- MAGIC                         channel: str = 'PREVIEW',
-- MAGIC                         development: bool = True,
-- MAGIC                         pipeline_type = 'WORKSPACE'
-- MAGIC                         ):
-- MAGIC   
-- MAGIC     '''
-- MAGIC   Creates the specified DLT pipeline.
-- MAGIC
-- MAGIC   Parameters:
-- MAGIC   ----------
-- MAGIC   pipeline_name : str
-- MAGIC       The name of the DLT pipeline to be created.
-- MAGIC   root_path_folder_name : str
-- MAGIC       The root folder name where the pipeline will be located. This folder must be in the location where this function is called.
-- MAGIC   source_folder_names : list, optional
-- MAGIC       A list of source folder names. Must defined at least one folder within the root folder location above.
-- MAGIC   catalog_name : str, optional
-- MAGIC       The catalog name for the DLT pipeline. Default is 'dbacademy'.
-- MAGIC   schema_name : str, optional
-- MAGIC       The schema name for the DLT pipeline. Default is 'default'.
-- MAGIC   serverless : bool, optional
-- MAGIC       If True, the pipeline will be serverless. Default is True.
-- MAGIC   configuration : dict, optional
-- MAGIC       A dictionary of configuration settings for the pipeline. Default is an empty dictionary.
-- MAGIC   continuous : bool, optional
-- MAGIC       If True, the pipeline will be run in continuous mode. Default is False.
-- MAGIC   photon : bool, optional
-- MAGIC       If True, the pipeline will use Photon for processing. Default is True.
-- MAGIC   channel : str, optional
-- MAGIC       The channel for the pipeline, such as 'PREVIEW'. Default is 'PREVIEW'.
-- MAGIC   development : bool, optional
-- MAGIC       If True, the pipeline will be set up for development. Default is True.
-- MAGIC   pipeline_type : str, optional
-- MAGIC       The type of the pipeline (e.g., 'WORKSPACE'). Default is 'WORKSPACE'.
-- MAGIC
-- MAGIC   Returns:
-- MAGIC   -------
-- MAGIC   None
-- MAGIC       This function does not return anything. It creates the DLT pipeline based on the provided parameters.
-- MAGIC
-- MAGIC   Example:
-- MAGIC   --------
-- MAGIC   create_dlt_pipeline(pipeline_name='my_pipeline_name', 
-- MAGIC                       root_path_folder_name='6 - Putting a DLT Pipeline in Production Project',
-- MAGIC                       source_folder_names=['orders', 'status'])
-- MAGIC   '''
-- MAGIC   
-- MAGIC     w = WorkspaceClient()
-- MAGIC     for pipeline in w.pipelines.list_pipelines():
-- MAGIC         if pipeline.name == pipeline_name:
-- MAGIC             raise ValueError(f"Lakeflow Declarative Pipeline name '{pipeline_name}' already exists. Please delete the pipeline using the UI and rerun the cell to recreate the pipeline.")
-- MAGIC
-- MAGIC     ## Create empty dictionary
-- MAGIC     create_dlt_pipeline_call = {}
-- MAGIC
-- MAGIC     ## Pipeline type
-- MAGIC     create_dlt_pipeline_call['pipeline_type'] = pipeline_type
-- MAGIC
-- MAGIC     ## Modify dictionary for specific DLT configurations
-- MAGIC     create_dlt_pipeline_call['name'] = pipeline_name
-- MAGIC
-- MAGIC     ## Set paths to root and source folders
-- MAGIC     main_course_folder_path = os.getcwd()
-- MAGIC
-- MAGIC     main_path_to_dlt_project_folder = os.path.join('/', main_course_folder_path, root_path_folder_name)
-- MAGIC     create_dlt_pipeline_call['root_path'] = main_path_to_dlt_project_folder
-- MAGIC
-- MAGIC     ## Add path of root folder to source folder names
-- MAGIC     add_path_to_folder_names = [os.path.join(main_path_to_dlt_project_folder, folder_name, '**') for folder_name in source_folder_names]
-- MAGIC     source_folders_path = [{'glob':{'include':folder_name}} for folder_name in add_path_to_folder_names]
-- MAGIC     create_dlt_pipeline_call['libraries'] = source_folders_path
-- MAGIC
-- MAGIC     ## Set default catalog and schema
-- MAGIC     create_dlt_pipeline_call['catalog'] = catalog_name
-- MAGIC     create_dlt_pipeline_call['schema'] = schema_name
-- MAGIC
-- MAGIC     ## Set serverless compute
-- MAGIC     create_dlt_pipeline_call['serverless'] = serverless
-- MAGIC
-- MAGIC     ## Set configuration parameters
-- MAGIC     create_dlt_pipeline_call['configuration'] = configuration
-- MAGIC
-- MAGIC     ## Set if continouous or not
-- MAGIC     create_dlt_pipeline_call['continuous'] = continuous 
-- MAGIC
-- MAGIC     ## Set to use Photon
-- MAGIC     create_dlt_pipeline_call['photon'] = photon
-- MAGIC
-- MAGIC     ## Set DLT channel
-- MAGIC     create_dlt_pipeline_call['channel'] = channel
-- MAGIC
-- MAGIC     ## Set if development mode
-- MAGIC     create_dlt_pipeline_call['development'] = development
-- MAGIC
-- MAGIC     ## Creat DLT pipeline
-- MAGIC
-- MAGIC     print(f"Creating the Lakeflow Declarative Pipeline '{pipeline_name}'...")
-- MAGIC     print(f"Root folder path: {main_path_to_dlt_project_folder}")
-- MAGIC     print(f"Source folder path(s): {source_folders_path}")
-- MAGIC
-- MAGIC     w.api_client.do('POST', '/api/2.0/pipelines', body=create_dlt_pipeline_call)
-- MAGIC     print(f"\nLakeflow Declarative Pipeline Creation '{pipeline_name}' Complete!")

-- COMMAND ----------

-- MAGIC %python
-- MAGIC catalog_name = "pipeline"
-- MAGIC schema_name = ['pipeline_data', '1_bronze_db', '2_silver_db', '3_gold_db']
-- MAGIC volume_name = "data"
-- MAGIC data_volume_path = f"/Volumes/{catalog_name}/{schema_name[0]}/{volume_name}"
-- MAGIC working_dir = data_volume_path
-- MAGIC print(working_dir)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Run the cell below to programmatically view the files in your `/Volumes/pipeline/pipeline_data/data/orders` volume. Confirm you only see the original **00.json** file in the **orders** folder.

-- COMMAND ----------

-- MAGIC %python
-- MAGIC spark.sql(f'LIST "{working_dir}/orders"').display()

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## B. Adding Data Quality Expectations
-- MAGIC
-- MAGIC This demonstration includes the simple starter Spark Declarative Pipeline that has already been created in the previous demonstration. We will continue to build on it to explore it's capabilities.

-- COMMAND ----------

-- MAGIC %md
-- MAGIC
-- MAGIC 1. Run the cell below to create your starter pipeline for this demonstration. The pipeline will set the following for you:
-- MAGIC
-- MAGIC     - Your default catalog: `pipeline`
-- MAGIC
-- MAGIC     - Your configuration parameter: `source` = `/Volumes/pipeline/pipeline_data/data`
-- MAGIC
-- MAGIC       **NOTE:** If the pipeline already exists, an error will be returned. In that case, you'll need to delete the existing pipeline and rerun this cell.
-- MAGIC
-- MAGIC       To delete the pipeline:
-- MAGIC
-- MAGIC       - Select **Jobs and Pipelines** from the far-left navigation bar.  
-- MAGIC
-- MAGIC       - Find the pipeline you want to delete.  
-- MAGIC
-- MAGIC       - Click the three-dot menu ![ellipsis icon](./Includes/images/ellipsis_icon.png).  
-- MAGIC
-- MAGIC       - Select **Delete**.
-- MAGIC
-- MAGIC **NOTE:**  The `create_declarative_pipeline` function is a custom function built for this course to create the sample pipeline using the Databricks REST API. This avoids manually creating the pipeline and referencing the pipeline assets.

-- COMMAND ----------

-- MAGIC %python
-- MAGIC create_declarative_pipeline(pipeline_name=f'3 - Adding Data Quality Expectations Project - {catalog_name}', 
-- MAGIC                             root_path_folder_name='3 - Adding Data Quality Expectations Project',
-- MAGIC                             catalog_name = catalog_name,
-- MAGIC                             schema_name = 'default',
-- MAGIC                             source_folder_names=['orders'],
-- MAGIC                             configuration = {'source':working_dir})

-- COMMAND ----------

-- MAGIC %md
-- MAGIC 2. Complete the following steps to open the starter Spark Declarative Pipeline project for this demonstration:
-- MAGIC
-- MAGIC    a. In the main navigation bar right-click on **Jobs & Pipelines** and select **Open in Link in New Tab**.
-- MAGIC
-- MAGIC    b. In **Jobs & Pipelines** select your **3 - Adding Data Quality Expectations Project - labuser** pipeline.
-- MAGIC
-- MAGIC    c. **REQUIRED:** At the top near your pipeline name, turn on **New pipeline monitoring**.
-- MAGIC
-- MAGIC    d. In the **Pipeline details** pane on the far right, select **Open in Editor** (field to the right of **Source code**) to open the pipeline in the **Lakeflow Pipeline Editor**.
-- MAGIC
-- MAGIC    e. In the new tab:
-- MAGIC       - Select the **orders** folder (The main folder also contains the extra **python_excluded** folder that contains the Python version)
-- MAGIC
-- MAGIC       - Click on **orders_pipeline.sql**.
-- MAGIC
-- MAGIC
-- MAGIC    f. In the navigation pane of the new tab, you should see **Pipeline** and **All Files**. Ensure you are in the **Pipeline** tab. This will list all files in your pipeline.
-- MAGIC    <br></br>
-- MAGIC
-- MAGIC #### IMPORTANT
-- MAGIC    **NOTE:** If you open the **orders_pipeline.sql** file and it does not open up the pipeline editor, that is because that folder is not associated with a pipeline. Please make sure to run the previous cell to associate the folder with the pipeline and try again.
-- MAGIC
-- MAGIC    **WARNING:** If you get the following warning when opening the **orders_pipeline.sql** file: 
-- MAGIC
-- MAGIC    ```pipeline you are trying to access does not exist or is inaccessible. Please verify the pipeline ID, request access or detach this file from the pipeline.``` 
-- MAGIC
-- MAGIC    Simply refresh the page and/or reselect the notebook.

-- COMMAND ----------

-- MAGIC %md
-- MAGIC 3. In the new tab, follow the instructions provided in the comments within the **orders_pipeline.sql** file.
-- MAGIC
-- MAGIC
-- MAGIC **Final Pipeline After Execution**
-- MAGIC ![Demo 3 Run](./Includes/images/demo3_run.png)
-- MAGIC

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Additional Resources
-- MAGIC
-- MAGIC - [Manage data quality with pipeline expectations](https://docs.databricks.com/aws/en/dlt/expectations)
-- MAGIC
-- MAGIC - [Expectation recommendations and advanced patterns](https://docs.databricks.com/aws/en/dlt/expectation-patterns)
-- MAGIC
-- MAGIC - [Data Quality Management With Databricks](https://www.databricks.com/discover/pages/data-quality-management#expectations-with-delta-live-tables)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC &copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href="https://www.apache.org/" target="_blank">Apache Software Foundation</a>.<br/><br/><a href="https://databricks.com/privacy-policy" target="_blank">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use" target="_blank">Terms of Use</a> | <a href="https://help.databricks.com/" target="_blank">Support</a>
