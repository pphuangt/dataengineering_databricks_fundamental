{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "5e4ffbfa-8e05-497b-9477-ed84950af344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Getting started with Delta Lake\n",
    "\n",
    "<img src=\"https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-logo-whitebackground.png\" style=\"width:200px; float: right\"/>\n",
    "\n",
    "[Delta Lake](https://delta.io/) is an open storage format used to save your data in your Lakehouse. Delta provides an abstraction layer on top of files. It's the storage foundation of your Lakehouse.\n",
    "\n",
    "In this notebook, we will explore Delta Lake main capabilities, from table creation to time travel.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=2940815195285158&notebook=%2F01-Getting-Started-With-Delta-Lake&demo_name=delta-lake&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdelta-lake%2F01-Getting-Started-With-Delta-Lake&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6984345-8a09-4598-aacf-61b34281f716",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Init the demo data under ${raw_data_location}/user_parquet."
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f49a8ca-6c28-4d71-b469-a54cc1c86a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#For this demo, We'll use a synthetic dataset containing user information, saved under ${raw_data_location}/user_parquet.\n",
    "print(f\"Our user dataset is stored under our Volume={folder}/user_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c236a04-704a-49c3-830c-903413da6701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Creating our first Delta Lake table\n",
    "\n",
    "Delta is the default file and table format using Databricks. You are likely already using delta without knowing it!\n",
    "\n",
    "Let's create a few table to see how to use Delta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a0e9665-14e9-4634-850f-8653590170a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Delta table using SQL"
    }
   },
   "outputs": [],
   "source": [
    "-- Creating our first Delta table\n",
    "CREATE TABLE IF NOT EXISTS user_delta (id BIGINT, creation_date TIMESTAMP, firstname STRING, lastname STRING, email STRING, address STRING, gender INT, age_group INT);\n",
    "\n",
    "-- Let's load some data in this table\n",
    "COPY INTO user_delta FROM '/Volumes/main/dbdemos_delta_lake/delta_lake_raw_data/user_parquet/' FILEFORMAT = parquet;\n",
    "\n",
    "SELECT * FROM user_delta;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f95a030b-5068-4477-95ac-a8f2d7dc74c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "That's it! Our Delta table is ready and you get all the Delta Benefits. \n",
    "\n",
    "Using Delta is that simple!\n",
    "\n",
    "Let's see how we can use Python or scala API to do the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce48ed91-b6bc-4340-a000-03bb22a845b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Delta table using python / Scala API"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "data_parquet = spark.read.parquet(folder+\"/user_parquet\")\n",
    "\n",
    "data_parquet.write.mode(\"overwrite\").saveAsTable('user_delta')\n",
    "\n",
    "display(spark.read.table('user_delta'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8761e242-0560-4167-ab77-3b7f37e73c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Upgrading an existing Parquet or Iceberg table to Delta Lake\n",
    "It's also very simple to migrate an existing Parquet or Iceberg table to Delta Lake. Here is an example: \n",
    "\n",
    "```\n",
    "CONVERT TO DELTA database_name.table_name; -- only for Parquet tables\n",
    "\n",
    "CONVERT TO DELTA parquet.`s3://my-bucket/path/to/table`\n",
    "  PARTITIONED BY (date DATE); -- if the table is partitioned\n",
    "\n",
    "CONVERT TO DELTA iceberg.`s3://my-bucket/path/to/table`; -- uses Iceberg manifest for metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78df533a-43e5-42e7-8e3c-219ab50004cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Delta Lake for Batch and Streaming operations\n",
    "\n",
    "Delta makes it super easy to work with data stream. \n",
    "\n",
    "In this example, we'll create a streaming query on top of our table, and add data in the table. The stream will pick the changes without any issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c1deedc-0ac5-4938-bad8-d050fdaa5ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Read the insertion of data\n",
    "spark.readStream.option(\"ignoreDeletes\", \"true\").option(\"ignoreChanges\", \"true\").table(\"user_delta\").createOrReplaceTempView(\"user_delta_readStream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf47443-cdf2-4872-a3c1-25fc60bd4c8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python \n",
    "df = spark.sql(\"select gender, round(avg(age_group),2) from user_delta_readStream group by gender\")\n",
    "# checkpointLocation option temporary required for serverless workspaces\n",
    "display(df, checkpointLocation = get_chkp_folder(folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d166cce3-cb70-4480-a58f-86b95b659f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wait** until the stream is up and running before executing the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa14e603-b8b0-465a-9a8c-e48626db62fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's add a new user, it'll appear in our previous aggregation as the stream picks up the update"
    }
   },
   "outputs": [],
   "source": [
    "insert into user_delta (id, creation_date, firstname, lastname, email, address, gender, age_group) \n",
    "    values (99999, now(), 'Quentin', 'Ambard', 'quentin.ambard@databricks.com', 'FR', '2', 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "252b6768-7ad2-476d-8e9f-33607efeaf8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Full DML Support\n",
    "\n",
    "Delta Lake supports standard DML including UPDATE, DELETE and MERGE INTO, providing developers more controls to manage their big datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31848a0-2920-4e7d-8f3a-d686e567c797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Running `UPDATE` on the Delta Lake table\n",
    "UPDATE user_delta SET age_group = 4 WHERE id = 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eff9ee1-e8d2-4135-9d48-8e37ed35e3cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DELETE FROM user_delta WHERE id = 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af3de993-d273-4451-af97-c113722e1393",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UPSERT: update if exists, insert otherwise"
    }
   },
   "outputs": [],
   "source": [
    "-- Let's create a table containing a list of changes we want to apply to the user table (ex: CDC flow)\n",
    "create table if not exists user_updates \n",
    "  (id bigint, creation_date TIMESTAMP, firstname string, lastname string, email string, address string, gender int, age_group int);\n",
    "  \n",
    "delete from user_updates;\n",
    "\n",
    "insert into user_updates values (1,     now(), 'Marco',   'polo',   'marco@polo.com',    'US', 2, 3); \n",
    "insert into user_updates values (2,     now(), 'John',    'Doe',    'john@doe.com',      'US', 2, 3);\n",
    "insert into user_updates values (99999, now(), 'Quentin', 'Ambard', 'qa@databricks.com', 'FR', 2, 3);\n",
    "select * from user_updates;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b44d70fe-9c9c-43ea-beaf-b3d12573f4f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- We can now MERGE the changes into our main table (note: we could also DELETE the rows based on a predicate)\n",
    "MERGE INTO user_delta as d USING user_updates as m\n",
    "  ON d.id = m.id\n",
    "  WHEN MATCHED THEN \n",
    "    UPDATE SET *\n",
    "  WHEN NOT MATCHED \n",
    "    THEN INSERT * ;\n",
    "  \n",
    "select * from user_delta where id in (1 ,2, 99999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e77b975d-912d-49e7-a2de-a601cce7f244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Enforce Data Quality with constraint\n",
    "\n",
    "Delta Lake support constraints. You can add any expression to force your table having a given field respecting this constraint. As example, let's make sure that the ID is never null.\n",
    "\n",
    "*Note: This is enforcing quality at the table level. Spark Declarative Pipelines offer much more advance quality rules and expectations in data Pipelines.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4ca8b8b-f543-462e-838e-3105ad45da87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ALTER TABLE user_delta DROP CONSTRAINT IF EXISTS id_not_null; ALTER TABLE user_delta ADD CONSTRAINT id_not_null CHECK (id is not null);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c452b5-228e-43eb-8c3b-8a3632ee9109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- This command will fail as we insert a user with a null id::\n",
    "INSERT INTO user_delta (id, creation_date, firstname, lastname, email, address, gender, age_group) \n",
    "                VALUES (null, now(), 'Quentin', 'Ambard', 'quentin.ambard@databricks.com', 'FR', '2', 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0bc413b-c0bb-4667-b4c6-96dd33aec5a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Let's Travel back in Time!\n",
    "Databricks Delta’s time travel capabilities simplify building data pipelines for the following use cases. \n",
    "\n",
    "* Audit Data Changes\n",
    "* Reproduce experiments & reports\n",
    "* Rollbacks\n",
    "\n",
    "As you write into a Delta table or directory, every operation is automatically versioned.\n",
    "\n",
    "You can query a table by:\n",
    "1. Using a timestamp\n",
    "1. Using a version number\n",
    "\n",
    "For more information, refer to [Introducing Delta Time Travel for Large Scale Data Lakes](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79757609-32ad-4926-b01b-c538967ef02e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Review Delta Lake Table History\n",
    "All the transactions for this table are stored within this table including the initial set of insertions, update, delete, merge, and inserts with schema modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c62f55b8-d984-41e3-af0e-10c9830ca184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY user_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e60ad09-00d4-426f-96d1-4c3c7321d4c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Time Travel via Version Number or Timestamp\n",
    "Below are SQL syntax examples of Delta Time Travel by using a Version Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52b96c31-d15b-4288-b60b-8940e78619d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Note that in our current version, user 99999 exists and we updated user 1 and 2\n",
    "SELECT * FROM user_delta WHERE ID IN (1 ,2, 99999);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ff46056-6ea1-40d5-afa9-8eaae4b3e3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- We can request the table at version 2, before the upsert operation to get the original data:\n",
    "SELECT * FROM user_delta VERSION AS OF 2 WHERE ID IN (1 ,2, 99999);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdd834f3-a74f-4ff5-a731-a59e06ce7dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Restore a Previous Version\n",
    "You can restore a Delta table to its earlier state by using the `RESTORE` command, using a timestamp or delta version:\n",
    "\n",
    "⚠️ Databricks Runtime 7.4 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "619dd119-46e4-4e0c-99da-a85c1ed7acad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RESTORE TABLE user_delta TO VERSION AS OF 2;\n",
    "\n",
    "SELECT * FROM user_delta WHERE ID IN (1 ,2, 99999);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed29cdfe-d9a6-4b5e-b9cc-ff62be45cd35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Purging the history with VACUUM"
    }
   },
   "outputs": [],
   "source": [
    "-- We can easily delete all modification older than 200 hours:\n",
    "VACUUM user_delta RETAIN 200 HOURS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2827a08c-8202-4cbf-90e4-a6dd412bf14a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CLONE Delta Tables\n",
    "You can create a copy of an existing Delta table at a specific version using the `clone` command. This is very useful to get data from a PROD environment to a STAGING one, or archive a specific version for regulatory reason.\n",
    "\n",
    "There are two types of clones:\n",
    "* A **deep clone** is a clone that copies the source table data to the clone target in addition to the metadata of the existing table. \n",
    "* A **shallow clone** is a clone that does not copy the data files to the clone target. The table metadata is equivalent to the source. These clones are cheaper to create.\n",
    "\n",
    "Any changes made to either deep or shallow clones affect only the clones themselves and not the source table.\n",
    "\n",
    "*Note: Shallow clone are pointers to the main table. Running a VACUUM may delete the underlying files and break the shallow clone!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8869bccd-79f4-4117-9495-63a2b2b72bc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Shallow clone (zero copy)"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS user_delta_clone\n",
    "  SHALLOW CLONE user_delta\n",
    "  VERSION AS OF 2;\n",
    "\n",
    "SELECT * FROM user_delta_clone;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e18bfd7-20e2-468d-ae21-65e291fc99d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deep clone (copy data)"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS user_delta_clone_deep\n",
    "  DEEP CLONE user_delta;\n",
    "\n",
    "SELECT * FROM user_delta_clone_deep;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e6c78eb-d4ab-4c73-a00b-e3caa4c1b671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generated columns\n",
    "\n",
    "Delta Lake makes it easy to add auto increment columns. This is done with the `GENERATED` keyword. Generated value can also be derivated from other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5227acb-4690-460f-ad60-1dd73b9e3e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS user_delta_generated_id (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY ( START WITH 10000 INCREMENT BY 1 ), \n",
    "  firstname STRING, \n",
    "  lastname STRING, \n",
    "  email STRING, \n",
    "  address STRING) ;\n",
    "\n",
    "-- Note that we don't insert data for the id. The engine will handle that for us:\n",
    "INSERT INTO user_delta_generated_id (firstname, lastname, email, address) SELECT\n",
    "    firstname,\n",
    "    lastname,\n",
    "    email,\n",
    "    address\n",
    "  FROM user_delta;\n",
    "\n",
    "-- The ID is automatically generated!\n",
    "SELECT * from user_delta_generated_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2627fa09-95a8-4ba9-923a-b38640c1c34f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "DBDemos.stop_all_streams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2bc65a2-c68c-46b8-bf2b-319314f6a321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Going further with Delta Lake\n",
    "\n",
    "### Schema evolution \n",
    "\n",
    "Delta Lake support schema evolution. You can add a new column and even more advanced operation such as [updating partition](https://docs.databricks.com/delta/delta-batch.html#change-column-type-or-name). For SQL MERGE operation you easily add new columns with `SET spark.databricks.delta.schema.autoMerge.enabled = true`\n",
    "\n",
    "More details from the [documentation](https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-alter-table.html). \n",
    "\n",
    "### Identity columns, PK & FK \n",
    "\n",
    "You can add auto-increment columns in your tables: `id BIGINT GENERATED ALWAYS AS IDENTITY ( START WITH 0 INCREMENT BY 1 )`, but also define Primary Keys and Foreign Keys.  \n",
    "\n",
    "For more details, check our demo `dbdemos.install('identity-pk-fk')` !\n",
    "\n",
    "### Delta Sharing, open data sharing protocol\n",
    "\n",
    "[Delta Sharing](https://delta.io/sharing/) is an open standard to easily share your tables with external organization, using Databricks or any other system / cloud provider.\n",
    "\n",
    "For more details, check our demo `dbdemos.install('delta-sharing')` !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a67aebf-cd24-41b8-8546-7c169b343d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Next: Discover how to boost your queries with [Delta Lake performance features]($./02-Delta-Lake-Performance) or go back to [00-Delta-Lake-Introduction]($./00-Delta-Lake-Introduction).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-Getting-Started-With-Delta-Lake",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
