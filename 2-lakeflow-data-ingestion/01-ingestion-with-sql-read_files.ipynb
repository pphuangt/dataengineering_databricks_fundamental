{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f363fa5a-8376-4fec-9531-7c1fdb0f3c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# What is Databricks SQL's read_files function?\n",
    "The `read_files` function lets you directly query and ingest files (like `CSV`, `JSON`, `Parquet`, etc.) from cloud storage or Unity Catalog using SQL‚Äîno table setup needed.\n",
    "\n",
    "It supports ad-hoc exploration and incremental ingestion, including with `STREAMING TABLES`, and automatically infers schema and handles directories or patterns.\n",
    "\n",
    "*Note: Spark Declarative Pipelines use read_files to easily load file data into tables, powering both batch and streaming workflows.*\n",
    "\n",
    "For more details, open [the read_files documentation](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bfde718-3d36-4188-ab33-7b10bf9fd03f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data initialization - run the cell to prepare the demo data."
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed64d852-83fb-47d6-a649-a794e4dd1e76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(volume_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55807dc0-1e00-4ef1-81bc-9bf2c0802d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1. Basic Usage: Automatic Format Dectection\n",
    "\n",
    "One of the key advantages of `read_files` is automatic format detection. Let's try to read many file formats in our demo data folder, and use read_files to detect the file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4b627fc-9aee-496b-a29e-1683a76e440a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading JSON files (auto-detected)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_json') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0601b5bc-037f-41c6-8f66-31b85d1c3e21",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading CSV files (auto-detected)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_csv') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "141c302b-0ed4-4bbd-85d4-32096bb4320a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading parquet files (auto-detected)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_parquet') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57a7145e-7131-4458-a3d6-ab0e79d1469f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Partitioned Parquet auto-detection"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT year, month, COUNT(*) as records FROM read_files('{volume_folder}/user_parquet_partitioned') GROUP BY year, month\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c4f3a6-6861-4145-b226-a341519f01f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "`read_files` also supports powerful glob patterns for selective file reading. You can select the specific format you want to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10a3f012-a5eb-4a46-af08-5f68d7de8af1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Basic glob patterns"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT 'JSON Files' as source, * FROM read_files('{volume_folder}/*json*') LIMIT 3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64f2b38d-047f-448e-96e4-810b7f4ffe2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2. Schema Inference\n",
    "\n",
    "Different formats have different schema inference capabilities and performance.\n",
    "\n",
    "We can also use schema hints to override the schema inferrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d00ad6b-f24f-4afb-aab2-6c14c9ab64d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON schema (inferred from data)"
    }
   },
   "outputs": [],
   "source": [
    "json_schema = spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_json') LIMIT 0\").schema\n",
    "print(json_schema.treeString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58dbdf6b-c4e2-4370-b93d-3aaac000c692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV schema (inferred with headers)"
    }
   },
   "outputs": [],
   "source": [
    "csv_schema = spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_csv') LIMIT 0\").schema  \n",
    "print(csv_schema.treeString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c11f627b-4399-47d0-90a6-d66bbc4d5e71",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Type Precision Comparison"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  format,\n",
    "  MAX(id_type) AS id_type,\n",
    "  MAX(age_group_type) AS age_group_type,\n",
    "  MAX(date_type) AS date_type\n",
    "FROM (\n",
    "SELECT \n",
    "  'JSON' as format,\n",
    "  typeof(id) as id_type,\n",
    "  typeof(age_group) as age_group_type,\n",
    "  typeof(creation_date) as date_type\n",
    "FROM read_files('{volume_folder}/user_json')\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'CSV' as format,\n",
    "  typeof(id) as id_type, \n",
    "  typeof(age_group) as age_group_type,\n",
    "  typeof(creation_date) as date_type\n",
    "FROM read_files('{volume_folder}/user_csv')\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'Parquet' as format,\n",
    "  typeof(id) as id_type,\n",
    "  typeof(age_group) as age_group_type, \n",
    "  typeof(creation_date) as date_type\n",
    "FROM read_files('{volume_folder}/user_parquet')\n",
    ") type_comparision\n",
    "GROUP BY format\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fb1aa02-35cc-4cb0-bf93-fd00179218ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using schema hints to override JSON inference"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  id,\n",
    "  typeof(id) as id_type_after_hint,\n",
    "  age_group,\n",
    "  typeof(age_group) as age_group_type_after_hint\n",
    "FROM read_files(\n",
    "  '{volume_folder}/user_json',\n",
    "  schemaHints => 'id bigint, age_group string'\n",
    ") LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "053f28a8-cd07-44a2-beeb-470cadf88dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3. Format-Specific Features\n",
    "\n",
    "There are some particular options that are specific to each format with `read_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "761a18de-695b-4f3b-8a66-2cad33f13fd0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV without headers"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_csv_no_headers', format => 'csv', header => 'false') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ce17e0a-6680-4ef2-b2d6-aea4c5ccf8bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV without headers (manual schema)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT * FROM read_files(\n",
    "  '{volume_folder}/user_csv_no_headers',\n",
    "  format => 'csv',\n",
    "  schema => 'id bigint, creation_date string, firstname string, lastname string, email string, address string, gender double, age_group double'\n",
    ") LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fcd6767-7463-4779-8f64-106aa123b88e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV with pipe delimiter"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT * FROM read_files(\n",
    "  '{volume_folder}/user_csv_pipe_delimited',\n",
    "  format => 'csv',\n",
    "  sep => '|'  \n",
    ") LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3716080-8f59-4465-94bd-0fa61ccd9752",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON with column type inference"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  firstname,\n",
    "  lastname,\n",
    "  id,\n",
    "  typeof(id) as id_inferred_type,\n",
    "  age_group,\n",
    "  typeof(age_group) as age_group_inferred_type\n",
    "FROM read_files(\n",
    "  '{volume_folder}/user_json',\n",
    "  inferColumnTypes => true\n",
    ") LIMIT 5  \n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f815a542-529e-4989-9893-75a163385286",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parquet optimization features"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrate column pruning (Parquet's key advantage)\n",
    "print(\"‚ö° Parquet Column Pruning Demo:\")\n",
    "import time\n",
    "\n",
    "# Read all columns\n",
    "start_time = time.time()\n",
    "all_cols_count = spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_parquet')\").count()\n",
    "all_cols_time = time.time() - start_time\n",
    "\n",
    "# Read only specific columns  \n",
    "start_time = time.time()\n",
    "select_cols_count = spark.sql(f\"SELECT id, firstname FROM read_files('{volume_folder}/user_parquet')\").count()\n",
    "select_cols_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìä All columns: {all_cols_count:,} records in {all_cols_time:.2f}s\")\n",
    "print(f\"üìä 2 columns: {select_cols_count:,} records in {select_cols_time:.2f}s\") \n",
    "print(f\"‚ö° Column pruning speedup: {all_cols_time/select_cols_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b94a3b29-a667-4c02-a94d-eb04da9bbeb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Streaming Usage\n",
    "\n",
    "`read_files` can be used in streaming tables to ingest files into Delta Lake. `read_files` leverages Auto Loader when used in a streaming table query.\n",
    "\n",
    "To do so, simply add the `STREAM` keyword to your SQL queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1311645-1624-42e7-b7f6-c0145df34299",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Basic streaming example"
    }
   },
   "outputs": [],
   "source": [
    "# Create a streaming view\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW streaming_json_users AS\n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() as processing_time\n",
    "FROM STREAM read_files(\n",
    "  '{volume_folder}/user_json',\n",
    "  maxFilesPerTrigger => 5,\n",
    "  schemaLocation => '{volume_folder}/read_files_streaming_schema'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "display(spark.sql(\"SELECT COUNT(*) as total_records FROM streaming_json_users\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "954cc3f9-afeb-40ff-b2f8-7071da30ac24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multi-format streaming (separate streams with schema locations)"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW streaming_csv_users AS  \n",
    "SELECT \n",
    "  *,\n",
    "  'CSV' as source_format,\n",
    "  current_timestamp() as processing_time\n",
    "FROM STREAM read_files(\n",
    "  '{volume_folder}/user_csv',\n",
    "  schemaLocation => '{volume_folder}/csv_streaming_schema'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW streaming_parquet_users AS\n",
    "SELECT \n",
    "  *,\n",
    "  'PARQUET' as source_format, \n",
    "  current_timestamp() as processing_time\n",
    "FROM STREAM read_files(\n",
    "  '{volume_folder}/user_parquet',\n",
    "  schemaLocation => '{volume_folder}/parquet_streaming_schema'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "display(spark.sql(\"\"\"\n",
    "SELECT 'CSV' as format, COUNT(*) as records FROM streaming_csv_users\n",
    "UNION ALL  \n",
    "SELECT 'PARQUET' as format, COUNT(*) as records FROM streaming_parquet_users\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e81b8f5-3609-49a4-b40d-5b8b588f9945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 5. read_files vs Auto Loader\n",
    "\n",
    "We have covered some basic features of `read_files`. However, there might be some questions about when to use `read_files` and when to use Auto Loader.\n",
    "\n",
    "We have some comparison and decision matrix that could help you decide when to leverage the power of `read_files` and Auto Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1abb17f-fd8a-4ebc-9da3-e531905b4f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "| Capability | read_files | Auto Loader |\n",
    "|-----------|------------|-------------|\n",
    "| Language | SQL | Python  |\n",
    "| Ad-hoc queries | ‚úÖ Perfect | Incremental Streaming focus  |\n",
    "| Batch processing | ‚úÖ Excellent | Incremental Streaming focus |\n",
    "| Multi-format API | ‚úÖ Unified API | Need to declare format |\n",
    "| Streaming performance | Optimized for you | Mode advanced options for more control |\n",
    "| Schema evolution | ‚ö†Ô∏è Manual | ‚úÖ Automatic |\n",
    "| Setup complexity | ‚úÖ Zero setup | Pythonic config |\n",
    "| File notifications | ‚ùå No | ‚úÖ Cloud notifications |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4954ca3-c12d-49ed-ab24-c7ffe3207c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "We have seen what the capabilities of Databricks SQL's `read_files` are, and now you can apply it in your projects.\n",
    "\n",
    "Open the [02-Auto-loader-schema-evolution-Ingestion]($./02-Auto-loader-schema-evolution-Ingestion) Notebook to explore the Auto Loader options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee6b0fb5-4af5-44ad-b5dc-8dd8968b5a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-ingestion-with-sql-read_files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
