{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "9573d6b1-be5b-4753-a266-555cd4d056cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# What is Databricks Auto Loader?\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/autoloader/autoloader-edited-anim.gif\" style=\"float:right; margin-left: 10px\" />\n",
    "\n",
    "[Databricks Auto Loader](https://docs.databricks.com/ingestion/auto-loader/index.html) lets you scan a cloud storage folder (S3, ADLS, GS) and only ingest the new data that arrived since the previous run.\n",
    "\n",
    "This is called **incremental ingestion**.\n",
    "\n",
    "Auto Loader can be used in a near real-time stream or in a batch fashion, e.g., running every night to ingest daily data.\n",
    "\n",
    "Auto Loader provides a strong gaurantee when used with a Delta sink (the data will only be ingested once).\n",
    "\n",
    "## How Auto Loader simplifies data ingestion\n",
    "\n",
    "Ingesting data at scale from cloud storage can be really hard at scale. Auto Loader makes it easy, offering these benefits:\n",
    "\n",
    "\n",
    "* **Incremental** & **cost-efficient** ingestion (removes unnecessary listing or state handling)\n",
    "* **Simple** and **resilient** operation: no tuning or manual code required\n",
    "* Scalable to **billions of files**\n",
    "  * Using incremental listing (deprecated, relies on filename order)\n",
    "  * Leveraging notification + message queue (recommended)\n",
    "* **Schema inference** and **schema evolution** are handled out of the box for most formats (csv, json, avro, images...)\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=2940815195285158&notebook=%2F02-Auto-loader-schema-evolution-Ingestion&demo_name=data-ingestion&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdata-ingestion%2F02-Auto-loader-schema-evolution-Ingestion&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "883cf927-4213-491d-ba5f-119b9d6cdc9f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data initialization - run the cell to prepare the demo data."
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ebb6d02-d00c-4837-bdd4-aabbad63a9c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's explore what is being delivered in our bucket: (json)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.text(volume_folder+'/user_json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c1ff1a-772a-4226-ac86-a9adfd0ada71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Auto Loader basics\n",
    "Let's create a new Auto Loader stream that will incrementally ingest new incoming files.\n",
    "\n",
    "In this example we will specify the full schema. We will also use `cloudFiles.maxFilesPerTrigger` to take 1 file a time to simulate a process adding files 1 by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58ec306e-b567-4d35-92d2-d3920687bf41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronzeDF = (spark.readStream \\\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"json\")\n",
    "                .option(\"cloudFiles.maxFilesPerTrigger\", \"1\")  #demo only, remove in real stream\n",
    "                .schema(\"address string, creation_date string, firstname string, lastname string, id bigint\")\n",
    "                .load(volume_folder+'/user_json'))\n",
    "display(bronzeDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b86f6711-a0a0-4b4a-91ad-a0433eed60b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Schema inference\n",
    "Specifying the schema manually can be a challenge, especially with dynamic JSON. Notice that we are missing the \"age\" data because we overlooked specifying this column in the schema.\n",
    "\n",
    "* Schema inference has always been expensive and slow at scale, but not with Auto Loader. Auto Loader efficiently samples data to infer the schema and stores it under `cloudFiles.schemaLocation` in your bucket. \n",
    "* Additionally, `cloudFiles.inferColumnTypes` will determine the proper data type from your JSON.\n",
    "\n",
    "Let's redefine our stream with these features. Notice that we now have all of the JSON fields.\n",
    "\n",
    "*Notes:*\n",
    "* *With Spark Declarative Pipelines you don't even have to set this option, the engine manages the schema location for you.*\n",
    "* *Sampling size can be changed with `spark.databricks.cloudFiles.schemaInference.sampleSize.numBytes`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ad18ed5-e5d8-4401-a008-9caf1a287f59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Auto Loader can now infer the schema automatically (from any format) "
    }
   },
   "outputs": [],
   "source": [
    "bronzeDF = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"json\")\n",
    "                .option(\"cloudFiles.schemaLocation\", volume_folder+'/inferred_schema')\n",
    "                .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                .load(volume_folder+'/user_json'))\n",
    "display(bronzeDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67da54e-a188-4f0b-8edb-74aa3e4fca84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Schema hints\n",
    "You might need to enforce a part of your schema, e.g., to convert a timestamp. This can easily be done with Schema Hints.\n",
    "\n",
    "In this case, we'll make sure that the `id` is read as `bigint` and not `int`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "933c8e45-bfb3-4540-8292-2cd5a37e86ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronzeDF = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"json\")\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"{volume_folder}/inferred_schema\")\n",
    "                .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                .option(\"cloudFiles.schemaHints\", \"id bigint\")\n",
    "                .load(volume_folder+'/user_json'))\n",
    "display(bronzeDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b305f84-0031-499d-80ec-3f7c4a03cb01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "890e454a-2862-4056-942d-79b5b9ea5a4b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schema evolution is now supported by restarting the stream"
    }
   },
   "outputs": [],
   "source": [
    "def get_stream():\n",
    "  return (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"json\")\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"{volume_folder}/inferred_schema\")\n",
    "                .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                .option(\"cloudFiles.schemaHints\", \"id bigint\")\n",
    "                .load(volume_folder+'/user_json'))\n",
    "display(get_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3349f684-8a6c-4205-9038-5fc09df4ea1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Incorrect schema\n",
    "Auto Loader automatically recovers from incorrect schema and conflicting type. It'll save incorrect data in the `_rescued_data` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66bcf6f8-f081-4b55-9b25-c692d57ee19d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Adding an incorrect field (\"id\" as string instead of bigint)"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "data = [Row(email=\"quentin.ambard@databricks.com\", firstname=\"Quentin\", id=\"456455\", lastname=\"Ambard\")]\n",
    "incorrect_data = spark.createDataFrame(data)\n",
    "incorrect_data.write.format(\"json\").mode(\"append\").save(volume_folder + \"/user_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22a7b81d-346c-4c6a-b42b-c5d0db501623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wait_for_rescued_data()\n",
    "# Start the stream and filter on on the rescue column to see how the incorrect data is captured\n",
    "display(get_stream().filter(\"_rescued_data is not null\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f018207-ef77-45f0-8e87-a332651cb2ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding a new column\n",
    "By default the stream will tigger a `UnknownFieldException` exception on new column. You then have to restart the stream to include the new column. \n",
    "\n",
    "Make sure your previous stream is still running and run the next cell.\n",
    "\n",
    "*Notes*:\n",
    "* *See `cloudFiles.schemaEvolutionMode` for different behaviors and more details.*\n",
    "* *Don't forget to add `.writeStream.option(\"mergeSchema\", \"true\")` to dynamically add when columns when writting to a delta table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c8b9b33-8358-4e07-88df-befb9fa3e064",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Adding a row with an extra column (\"new_column\":\"test new column value\")"
    }
   },
   "outputs": [],
   "source": [
    "# Stop all the existing streams\n",
    "DBDemos.stop_all_streams()\n",
    "\n",
    "# Add 'new_column'\n",
    "data = [Row(email=\"quentin.ambard@databricks.com\", firstname=\"Quentin\", id=456454, lastname=\"Ambard\", new_column=\"test new column value\")]\n",
    "new_row = spark.createDataFrame(data)\n",
    "new_row.write.format(\"json\").mode(\"append\").save(volume_folder + \"/user_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c393cf2-d47c-47e9-ab4e-a0a691b08554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Existing stream wil fail with: org.apache.spark.sql.catalyst.util.UnknownFieldException: Encountered unknown field(s) during parsing: {\"new_column\":\"test new column value\"}\n",
    "display(get_stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ede28590-44b3-4d27-9ac6-4718e2666a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We just have to restart it to capture the new data. Let's filter on the new column to make sure we have the proper row \n",
    "# (re-run the cell)\n",
    "display(get_stream().filter('new_column is not null'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "448412e9-b37e-498e-9a88-743f782bc430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingesting a high volume of input files\n",
    "Scanning folders with many files to detect new data is an expensive operation, leading to ingestion challenges and higher cloud storage costs.\n",
    "\n",
    "To solve this issue and support an efficient listing, Databricks autoloader offers two modes:\n",
    "\n",
    "- Incremental listing with `cloudFiles.useIncrementalListing` (deprecated), based on the alphabetical order of the file's path to only scan new data: (`ingestion_path/YYYY-MM-DD`)\n",
    "- Notification system, which sets up a managed cloud notification system sending new file name to a queue (recommended). See `cloudFiles.useNotifications` for more details.\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/autoloader-mode.png\" width=\"700\"/>\n",
    "\n",
    "Use the notification system option whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a23126-4990-428a-9643-46793788ef4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Support for images\n",
    "Databricks Auto Loader provides native support for images and binary files.\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/autoloader-images.png\" width=\"800\" />\n",
    "\n",
    "Just set the format accordingly and the engine will do the rest: `.option(\"cloudFiles.format\", \"binaryFile\")`\n",
    "\n",
    "Use-cases:\n",
    "\n",
    "- ETL images into a Delta table using Auto Loader\n",
    "- Automatically ingest continuously arriving new images\n",
    "- Easily retrain ML models on new images\n",
    "- Perform distributed inference using a pandas UDF directly from Delta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ba581ba-a49d-4c08-ad00-74fcd0ba4dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploying robust ingestion jobs in production\n",
    "\n",
    "Let's see how to use Auto Loader to ingest JSON files, support schema evolution, and automatically restart when a new column is found.\n",
    "\n",
    "If you need your job to be resilient with regard to an evolving schema, you have multiple options:\n",
    "\n",
    "* Let the full job fail & configure Databricks Workflow to restart it automatically\n",
    "* Leverage Spark Declarative Pipelines to simplify all the setup (SDP handles everything for you out of the box)\n",
    "* Wrap your call to restart the stream when the new column appears.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8246d062-7fbb-46ea-ba14-7968e64b9b5c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define helper functions"
    }
   },
   "outputs": [],
   "source": [
    "def start_stream_restart_on_schema_evolution():\n",
    "  while True:\n",
    "    try:\n",
    "      q = (spark.readStream\n",
    "                  .format(\"cloudFiles\")\n",
    "                  .option(\"cloudFiles.format\", \"json\")\n",
    "                  .option(\"cloudFiles.schemaLocation\", f\"{volume_folder}/inferred_schema\")\n",
    "                  .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                  .load(volume_folder+\"/user_json\")\n",
    "                .writeStream\n",
    "                  .toTable(\"autoloader_demo_output\",\n",
    "                           checkpointLocation=volume_folder+\"/checkpoint\",\n",
    "                           mergeSchema=True)\n",
    "          )\n",
    "      q.awaitTermination()\n",
    "      return q\n",
    "    except BaseException as e:\n",
    "      # Adding a new column will trigger an UnknownFieldException. In this case we just restart the stream:\n",
    "      if 'UNKNOWN_FIELD_EXCEPTION' in str(e):\n",
    "        print(f\"Going to restart stream after schema change:\\n{e}\")\n",
    "      else:\n",
    "        raise e\n",
    "\n",
    "#Careful - this will run forever do not forget to stop your job/notebook after you tried!\n",
    "# Seeing [INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED] ? Interactive serverless isn't designed for unlimited streaming. See https://docs.databricks.com/en/compute/serverless/limitations.html#streaming\n",
    "# Use a classic cluster, or you can use the writeStream.trigger(availableNow=True) option instead, or move your code to a Declarative Pipeline!\n",
    "#start_stream_restart_on_schema_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4c2d375-5cd5-4fba-9c7a-8d78cd2222c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We've seen how Databricks Auto Loader can be used to easily ingest your files, solving all ingestion challenges!\n",
    "\n",
    "You're ready to use it in your projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3f83ed-c029-4e26-aab5-a901340efb36",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stop all active stream"
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-Auto-loader-schema-evolution-Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
